import os
import glob
import time
import json
from google.cloud import documentai_v1 as documentai
from google.cloud import storage
from google.protobuf.json_format import MessageToDict
from pypdf import PdfReader, PdfWriter # PDF ë¶„í• ì„ ìœ„í•œ ë¼ì´ë¸ŒP
# --- 1. [í•„ìˆ˜] ì‚¬ìš©ì ì„¤ì • ë³€ìˆ˜ ---
# 1-1. ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ê²½ë¡œ (JSON)
# [ì‚¬ìš©ì ì„¤ì • ì™„ë£Œ]
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = r"D:\CJ\Project Manager\IDARS\IDARS PJT\document-ai-gcs-connector-key.json"

# 1-2. GCP í”„ë¡œì íŠ¸ ë° ë¦¬ì†ŒìŠ¤ ì •ë³´
PROJECT_ID = "concrete-fabric-465212-n3" # (ì´ì „ ì •ë³´ì—ì„œ í™•ì¸)
LOCATION = "us" # (ì´ì „ ì •ë³´ì—ì„œ í™•ì¸)
SPLITTER_PROCESSOR_ID = "87937bc0c460cb85" # (ì´ì „ ì •ë³´ì—ì„œ í™•ì¸)

# 1-3. ë¡œì»¬ í´ë” ê²½ë¡œ
# [ì‚¬ìš©ì ì„¤ì • ì™„ë£Œ]
LOCAL_INPUT_FOLDER = r"D:\CJ\Project Manager\IDARS\Data\Input_Documents_To_Split"
LOCAL_OUTPUT_FOLDER = r"D:\CJ\Project Manager\IDARS\Data\Extractor_Training_Data"

# 1-4. GCS ë²„í‚· ê²½ë¡œ (ì„ì‹œ ì²˜ë¦¬ìš©)
# [ì‚¬ìš©ì ì„¤ì • ì™„ë£Œ]
GCS_INPUT_BUCKET = "idars-splitter-input"
# [ê°€ì •] GCS Output ë²„í‚· ì´ë¦„ì„ 'idars-splitter-output'ìœ¼ë¡œ ê°€ì •í•©ë‹ˆë‹¤. 
# GCPì—ì„œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. (ë§Œì•½ ë‹¤ë¥´ë‹¤ë©´ ì´ ë¼ì¸ì„ ìˆ˜ì •í•˜ì„¸ìš”.)
GCS_OUTPUT_BUCKET = "idars-splitter-output" 

WEEKLY_BATCH_ID = f"batch_{time.strftime('%Y%m%d_%H%M%S')}" # ê³ ìœ  ë°°ì¹˜ ID
# --- [í•„ìˆ˜] ì„¤ì • ì¢…ë£Œ ---


def upload_files_to_gcs(local_path: str, bucket_name: str, gcs_prefix: str) -> list[str]:
    """ë¡œì»¬ í´ë”ì˜ ëª¨ë“  PDFë¥¼ GCSë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    print(f"[{time.strftime('%H:%M:%S')}] 1. ë¡œì»¬ íŒŒì¼ ì—…ë¡œë“œ ì‹œì‘...")
    storage_client = storage.Client(project=PROJECT_ID)
    bucket = storage_client.bucket(bucket_name)
    
    local_pdf_files = glob.glob(os.path.join(local_path, "*.pdf"))
    if not local_pdf_files:
        print(f"ğŸ›‘ ì˜¤ë¥˜: '{local_path}' í´ë”ì—ì„œ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []

    print(f"    ì´ {len(local_pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ GCSë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
    gcs_uris = []
    for local_file in local_pdf_files:
        file_name = os.path.basename(local_file)
        gcs_path = f"{gcs_prefix}/{file_name}"
        blob = bucket.blob(gcs_path)
        blob.upload_from_filename(local_file)
        gcs_uris.append(f"gs://{bucket_name}/{gcs_path}")

    print(f"[{time.strftime('%H:%M:%S')}] âœ… GCS ì—…ë¡œë“œ ì™„ë£Œ.")
    return gcs_uris

def run_splitter_batch_process(processor_id: str, gcs_input_uris: list[str], gcs_output_bucket: str, gcs_output_prefix: str) -> (str, str):
    """DMS Splitter ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìš”ì²­í•˜ê³ , ì™„ë£Œ í›„ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ GCS ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    print(f"[{time.strftime('%H:%M:%S')}] 2. DMS Splitter ë°°ì¹˜ ì²˜ë¦¬ ìš”ì²­...")
    
    docai_client = documentai.DocumentProcessorServiceClient(client_options={"api_endpoint": f"{LOCATION}-documentai.googleapis.com"})
    processor_name = docai_client.processor_path(PROJECT_ID, LOCATION, processor_id)

    input_documents = [documentai.GcsDocument(gcs_uri=uri, mime_type="application/pdf") for uri in gcs_input_uris]
    input_config = documentai.BatchDocumentsInputConfig(gcs_documents=documentai.GcsDocuments(documents=input_documents))
    
    output_gcs_uri = f"gs://{gcs_output_bucket}/{gcs_output_prefix}/"
    output_config = documentai.DocumentOutputConfig(gcs_output_config=documentai.DocumentOutputConfig.GcsOutputConfig(gcs_uri=output_gcs_uri))

    request = documentai.BatchProcessRequest(name=processor_name, input_documents=input_config, document_output_config=output_config)
    operation = docai_client.batch_process_documents(request)
    print(f"    ì‘ì—… ì‹œì‘ë¨. ì™„ë£Œê¹Œì§€ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
    
    try:
        operation.result(timeout=1800) # 30ë¶„
        print(f"[{time.strftime('%H:%M:%S')}] âœ… DMS Splitter ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ.")
        
        # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        manifest_gcs_path = save_batch_manifest(
            operation_metadata=operation.metadata,
            bucket_name=gcs_output_bucket,
            output_prefix=gcs_output_prefix
        )
        return output_gcs_uri, manifest_gcs_path
        
    except Exception as e:
        print(f"ğŸ›‘ ì˜¤ë¥˜: ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ - {e}")
        return None, None

def save_batch_manifest(operation_metadata: documentai.BatchProcessMetadata, bucket_name: str, output_prefix: str) -> str:
    """ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ í›„, ì…ë ¥/ì¶œë ¥ ë§¤í•‘ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ JSON íŒŒì¼ì„ GCSì— ì €ì¥í•©ë‹ˆë‹¤."""
    print(f"[{time.strftime('%H:%M:%S')}] 3. ë°°ì¹˜ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì‹œì‘...")
    storage_client = storage.Client(project=PROJECT_ID)
    mapping = {}
    metadata_dict = MessageToDict(operation_metadata._pb)
    
    for status in metadata_dict.get('individualProcessStatuses', []):
        try:
            input_uri = status.get('inputGcsSource')
            output_uri = status.get('outputGcsDestination') # ì˜ˆ: gs://.../1
            original_filename = os.path.basename(input_uri)
            output_folder_index = output_uri.strip('/').split('/')[-1]
            
            # [ë²„ê·¸ ìˆ˜ì •] 
            # output_uriê°€ gs://.../1 ì²˜ëŸ¼ '/' ì—†ì´ ëë‚˜ëŠ” ê²ƒì„ ê°€ì •í•˜ì—¬
            # output-document.json ì•ì— '/'ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
            json_gcs_path = f"{output_uri.rstrip('/')}/output-document.json"
            
            mapping[original_filename] = {
                "output_folder_index": output_folder_index,
                "output_json_gcs_path": json_gcs_path # ìˆ˜ì •ëœ ê²½ë¡œ ì €ì¥
            }
        except Exception:
            pass # ì˜¤ë¥˜ê°€ ìˆëŠ” íŒŒì¼ì€ ê±´ë„ˆëœë‹ˆë‹¤.

    if not mapping:
        print("ğŸ›‘ ì˜¤ë¥˜: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ë§¤í•‘ ì •ë³´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    try:
        bucket = storage_client.bucket(bucket_name)
        manifest_path = f"{output_prefix}/_batch_manifest.json"
        blob = bucket.blob(manifest_path)
        blob.upload_from_string(json.dumps(mapping, indent=2), content_type="application/json")
        print(f"[{time.strftime('%H:%M:%S')}] âœ… ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ ìƒì„± ì™„ë£Œ: gs://{bucket_name}/{manifest_path}")
        return f"gs://{bucket_name}/{manifest_path}"
    except Exception as e:
        print(f"ğŸ›‘ ì˜¤ë¥˜: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ GCS ì—…ë¡œë“œ ì‹¤íŒ¨ - {e}")
        return None

def download_parse_and_split_pdfs(manifest_gcs_path: str, local_input_folder: str, local_output_folder: str):
    """
    [ì‹ ê·œ ê¸°ëŠ¥]
    GCSì—ì„œ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì™€ JSON ê²°ê³¼ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³ , ì›ë³¸ PDFë¥¼ ìª¼ê°œì„œ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    print(f"[{time.strftime('%H:%M:%S')}] 4. PDF ë¶„í•  ë° ë¡œì»¬ ì €ì¥ ì‹œì‘...")
    storage_client = storage.Client(project=PROJECT_ID)
    
    # 1. ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±
    try:
        bucket_name, blob_name = manifest_gcs_path.replace("gs://", "").split("/", 1)
        bucket = storage_client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        manifest_data = json.loads(blob.download_as_string())
    except Exception as e:
        print(f"ğŸ›‘ ì˜¤ë¥˜: ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨. GCS ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {manifest_gcs_path} - {e}")
        return

    print(f"    ì´ {len(manifest_data)}ê°œì˜ ì›ë³¸ ë¬¸ì„œì— ëŒ€í•œ ë¶„í• ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # 2. ë§¤ë‹ˆí˜ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° íŒŒì¼ ì²˜ë¦¬
    for original_filename, info in manifest_data.items():
        try:
            # 3. GCSì—ì„œ Splitter ê²°ê³¼ JSON ë‹¤ìš´ë¡œë“œ
            json_gcs_path = info['output_json_gcs_path']
            json_bucket_name, json_blob_name = json_gcs_path.replace("gs://", "").split("/", 1)
            json_bucket = storage_client.bucket(json_bucket_name)
            json_blob = json_bucket.blob(json_blob_name)
            
            docai_result = json.loads(json_blob.download_as_string())
            
            # 4. ì›ë³¸ PDF íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            original_pdf_path = os.path.join(local_input_folder, original_filename)
            if not os.path.exists(original_pdf_path):
                print(f"    âš ï¸ ê²½ê³ : ì›ë³¸ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ (ê±´ë„ˆëœ€): {original_pdf_path}")
                continue
                
            reader = PdfReader(original_pdf_path)

            # 5. JSONì˜ 'entities' (ë¶„ë¥˜ëœ ë¬¸ì„œ)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ PDF ìª¼ê°œê¸°
            entities = docai_result.get('entities', [])
            print(f"    ğŸ“„ {original_filename}: ì´ {len(entities)}ê°œì˜ ì—”í‹°í‹° ê°ì§€ë¨")

            # ê°ì§€ëœ ëª¨ë“  ë¬¸ì„œ íƒ€ì… ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            detected_types = [entity.get('type', 'Unknown') for entity in entities]
            print(f"    ê°ì§€ëœ íƒ€ì…ë“¤: {detected_types}")

            for entity in entities:
                doc_type = entity.get('type', 'Etc') # ì˜ˆ: Bill_of_Lading, Commercial_Invoice, Packing_List

                # BLê³¼ Commercial Invoiceë§Œ í•„í„°ë§ (ì •í™•í•œ ë§¤ì¹­)
                doc_type_normalized = doc_type.replace('_', '').replace('-', '').replace(' ', '').lower()

                is_bl = doc_type_normalized == 'billoflading'
                is_ci = doc_type_normalized == 'commercialinvoice'

                if not (is_bl or is_ci):
                    print(f"    â­ï¸  ê±´ë„ˆë›°ê¸°: '{doc_type}' (BL/CIê°€ ì•„ë‹˜)")
                    continue

                # íƒ€ì…ëª… í‘œì¤€í™” (ì €ì¥ í´ë”ìš©)
                if is_bl:
                    standardized_type = "BL"
                elif is_ci:
                    standardized_type = "Commercial_Invoice"

                print(f"    ğŸ” '{doc_type}' â†’ '{standardized_type}' ì²˜ë¦¬ ì‹œì‘")

                # --- [ë²„ê·¸ ìˆ˜ì •] ---
                # í˜ì´ì§€ ë²”ìœ„ ì¶”ì¶œ ë¡œì§ ê°•í™” (ì—¬ëŸ¬ Document AI ì‘ë‹µ í˜•ì‹ ì§€ì›)
                page_anchor = entity.get('pageAnchor', {})

                # pageAnchorê°€ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
                if not page_anchor:
                    print(f"    âš ï¸ ê±´ë„ˆë›°ê¸°: '{standardized_type}' - pageAnchor ì—†ìŒ")
                    print(f"       [DEBUG] ì—”í‹°í‹° ì „ì²´: {json.dumps(entity, indent=2, ensure_ascii=False)}")
                    continue

                print(f"       [DEBUG] pageAnchor êµ¬ì¡°: {json.dumps(page_anchor, indent=2, ensure_ascii=False)}")

                page_start = None
                page_end = None

                # ì¼€ì´ìŠ¤ 1: pageSpans ì‚¬ìš© (start/end ë˜ëŠ” startIndex/endIndex)
                page_spans = page_anchor.get('pageSpans')
                if page_spans and len(page_spans) > 0:
                    span = page_spans[0]
                    print(f"       [DEBUG] pageSpans[0]: {json.dumps(span, indent=2, ensure_ascii=False)}")

                    # start/end í˜•ì‹ (ë¬¸ìì—´ì¼ ìˆ˜ë„ ìˆìŒ)
                    if 'start' in span:
                        page_start = int(span['start']) if span['start'] is not None else None
                        page_end = int(span.get('end', page_start + 1)) if span.get('end') is not None else page_start + 1
                    # startIndex/endIndex í˜•ì‹
                    elif 'startIndex' in span:
                        page_start = int(span['startIndex']) if span['startIndex'] is not None else None
                        page_end = int(span.get('endIndex', page_start + 1)) if span.get('endIndex') is not None else page_start + 1

                # ì¼€ì´ìŠ¤ 2: pageRefs ì‚¬ìš© (ê°œë³„ í˜ì´ì§€ ì°¸ì¡°)
                if page_start is None:
                    page_refs = page_anchor.get('pageRefs')
                    if page_refs:
                        print(f"       [DEBUG] pageRefs: {json.dumps(page_refs, indent=2, ensure_ascii=False)}")

                        # pageRefsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                        if isinstance(page_refs, list) and len(page_refs) > 0:
                            pages = []

                            for idx, ref in enumerate(page_refs):
                                # ë°©ë²• 1: ëª…ì‹œì ì¸ í˜ì´ì§€ ë²ˆí˜¸ í•„ë“œ
                                page_num = ref.get('page') or ref.get('pageNumber') or ref.get('pageIndex')

                                # ë°©ë²• 2: pageRefs ë°°ì—´ì˜ ì¸ë±ìŠ¤ê°€ í˜ì´ì§€ ë²ˆí˜¸ì¸ ê²½ìš°
                                # (confidenceë§Œ ìˆê³  page ë²ˆí˜¸ê°€ ì—†ì„ ë•Œ)
                                if page_num is None and 'confidence' in ref:
                                    page_num = idx  # ì¸ë±ìŠ¤ë¥¼ í˜ì´ì§€ ë²ˆí˜¸ë¡œ ì‚¬ìš©
                                    print(f"       [DEBUG] pageRefs[{idx}]ì— page ë²ˆí˜¸ ì—†ìŒ, ì¸ë±ìŠ¤ ì‚¬ìš©: {page_num}")

                                if page_num is not None:
                                    pages.append(int(page_num))

                            if pages:
                                page_start = min(pages)
                                page_end = max(pages) + 1  # exclusive end
                                print(f"       [DEBUG] pageRefsì—ì„œ ì¶”ì¶œí•œ í˜ì´ì§€: {pages} â†’ start={page_start}, end={page_end}")

                # ì¼€ì´ìŠ¤ 3: textAnchor ì‚¬ìš© (í…ìŠ¤íŠ¸ ìœ„ì¹˜ ê¸°ë°˜)
                if page_start is None:
                    text_anchor = entity.get('textAnchor')
                    if text_anchor:
                        print(f"       [DEBUG] textAnchor: {json.dumps(text_anchor, indent=2, ensure_ascii=False)}")

                        # textAnchor.textSegmentsì—ì„œ í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ ì‹œë„
                        text_segments = text_anchor.get('textSegments', [])
                        if text_segments:
                            # ì¼ë°˜ì ìœ¼ë¡œ ì²« ë²ˆì§¸ segmentì˜ ìœ„ì¹˜ë¡œ í˜ì´ì§€ íŒë‹¨
                            # í•˜ì§€ë§Œ ì´ê²ƒë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•  ìˆ˜ ìˆìŒ
                            print(f"       [DEBUG] textAnchorì— textSegments ìˆìŒ, í•˜ì§€ë§Œ í˜ì´ì§€ ë²ˆí˜¸ ì§ì ‘ ì¶”ì¶œ ë¶ˆê°€")

                # í˜ì´ì§€ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°
                if page_start is None or page_end is None:
                    print(f"    âš ï¸ ê±´ë„ˆë›°ê¸°: '{standardized_type}' - í˜ì´ì§€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨")
                    continue

                # í˜ì´ì§€ ë²”ìœ„ ìœ íš¨ì„± ê²€ì‚¬
                if page_start >= page_end or page_end > len(reader.pages):
                    print(f"    âš ï¸ ê±´ë„ˆë›°ê¸°: '{standardized_type}' - ìœ íš¨í•˜ì§€ ì•Šì€ í˜ì´ì§€ ë²”ìœ„ (Start: {page_start}, End: {page_end}, ì´ í˜ì´ì§€: {len(reader.pages)})")
                    continue

                print(f"    âœ… ì²˜ë¦¬ ì¤‘: '{standardized_type}' - í˜ì´ì§€ {page_start}~{page_end-1}")
                # --- [ë²„ê·¸ ìˆ˜ì • ì™„ë£Œ] ---

                # 6. ìƒˆ PDF íŒŒì¼ ìƒì„± (pypdf)
                writer = PdfWriter()
                for i in range(page_start, page_end):
                    writer.add_page(reader.pages[i])

                # 7. ìƒˆ ë¡œì»¬ í´ë”ì— ì €ì¥
                output_subfolder = os.path.join(local_output_folder, standardized_type)
                os.makedirs(output_subfolder, exist_ok=True)

                base_name = os.path.splitext(original_filename)[0]
                slip_number = base_name.split("_")[0] if "_" in base_name else base_name

                # íŒŒì¼ ì´ë¦„ ìƒì„± (1-based í˜ì´ì§€ ë²ˆí˜¸ë¡œ ë” ì§ê´€ì ìœ¼ë¡œ)
                # ë‹¨ì¼ í˜ì´ì§€: xxx_BL_page3.pdf
                # ì—¬ëŸ¬ í˜ì´ì§€: xxx_BL_page3-5.pdf
                page_start_1based = page_start + 1
                page_end_1based = page_end  # exclusive endì´ë¯€ë¡œ +1 ë¶ˆí•„ìš”

                if page_end - page_start == 1:
                    # ë‹¨ì¼ í˜ì´ì§€
                    new_filename = f"{slip_number}_{standardized_type}_page{page_start_1based}.pdf"
                else:
                    # ì—¬ëŸ¬ í˜ì´ì§€ (inclusive endë¡œ í‘œì‹œ)
                    new_filename = f"{slip_number}_{standardized_type}_page{page_start_1based}-{page_end_1based}.pdf"

                output_pdf_path = os.path.join(output_subfolder, new_filename)

                with open(output_pdf_path, "wb") as f_out:
                    writer.write(f_out)

                print(f"    ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_pdf_path}")

        except Exception as e:
            print(f"ğŸ›‘ ì˜¤ë¥˜: PDF ë¶„í•  ì¤‘ ì‹¤íŒ¨ (íŒŒì¼: {original_filename}) - {e}")


if __name__ == "__main__":
    if not os.path.exists(os.environ["GOOGLE_APPLICATION_CREDENTIALS"]):
        print(f"ğŸ›‘ ì˜¤ë¥˜: ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        exit()
        
    if not os.path.exists(LOCAL_INPUT_FOLDER):
        print(f"ğŸ›‘ ì˜¤ë¥˜: ë¡œì»¬ ì…ë ¥ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {LOCAL_INPUT_FOLDER}")
        exit()

    # 1. ë¡œì»¬ íŒŒì¼ ì—…ë¡œë“œ
    gcs_input_prefix = WEEKLY_BATCH_ID
    gcs_input_uris = upload_files_to_gcs(
        local_path=LOCAL_INPUT_FOLDER,
        bucket_name=GCS_INPUT_BUCKET,
        gcs_prefix=gcs_input_prefix
    )
    
    if gcs_input_uris:
        # 2. DMS Splitter ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        gcs_output_prefix = f"{WEEKLY_BATCH_ID}/1_splitter_results"
        output_gcs_folder, manifest_gcs_path = run_splitter_batch_process(
            processor_id=SPLITTER_PROCESSOR_ID,
            gcs_input_uris=gcs_input_uris,
            gcs_output_bucket=GCS_OUTPUT_BUCKET,
            gcs_output_prefix=gcs_output_prefix
        )
        
        # 3. [ì‹ ê·œ] PDF ë¶„í•  ë° ë¡œì»¬ ì €ì¥ ì‹¤í–‰
        if manifest_gcs_path:
            download_parse_and_split_pdfs(
                manifest_gcs_path=manifest_gcs_path,
                local_input_folder=LOCAL_INPUT_FOLDER,
                local_output_folder=LOCAL_OUTPUT_FOLDER
            )
            print("\n--- ëª¨ë“  ì‘ì—… ì™„ë£Œ ---")
            print(f"ìª¼ê°œì§„ PDF íŒŒì¼ì´ ë‹¤ìŒ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:")
            print(f"{LOCAL_OUTPUT_FOLDER}")
            print("ì´ì œ ì´ í´ë”ì˜ íŒŒì¼ë“¤ë¡œ Extractor í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
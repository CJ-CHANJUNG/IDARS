import fitz  # PyMuPDF
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import json
import re
from datetime import datetime
import pytesseract
from PIL import Image
import io

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”§ ì‚¬ìš©ì ì„¤ì • (ê²½ë¡œ ì„¤ì •)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# 1. [ê¸°ë³¸] ì›ë³¸ PDF ì…ë ¥ ë° ì „ì²´ ê²°ê³¼ ì¶œë ¥ ê²½ë¡œ
INPUT_FOLDER = r"D:\CJ\Project Manager\IDARS\ë¡œì»¬ ëª¨ë¸ í•™ìŠµ\Splitter\Input"
OUTPUT_FOLDER = r"D:\CJ\Project Manager\IDARS\ë¡œì»¬ ëª¨ë¸ í•™ìŠµ\Splitter\Output"

# 2. [Parser í•™ìŠµìš©] ë¶„ë¥˜ëœ íŒŒì¼ì´ ìë™ìœ¼ë¡œ ë“¤ì–´ê°ˆ ê²½ë¡œ
PARSER_CI_INPUT = r"D:\CJ\Project Manager\IDARS\ë¡œì»¬ ëª¨ë¸ í•™ìŠµ\Parser\CI_Input"
PARSER_BL_INPUT = r"D:\CJ\Project Manager\IDARS\ë¡œì»¬ ëª¨ë¸ í•™ìŠµ\Parser\BL_Input"

# 3. [í•„ìˆ˜] Tesseract OCR ê²½ë¡œ
TESSERACT_PATH = r'C:\Program Files\Tesseract-OCR\tesseract.exe'

# 4. ê¸°íƒ€ ì„¤ì •
MERGE_CONSECUTIVE_SAME_TYPE = True 
OCR_THRESHOLD = 100  # 1ì°¨ í•„í„° (ì´ë³´ë‹¤ ì ìœ¼ë©´ ë°”ë¡œ OCR)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ” íŒ¨í„´ ì •ì˜
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DOCUMENT_PATTERNS = {
    'Bill_of_Lading': [
        (r'BILL\s*OF\s*LADING', 100), (r'WAYBILL', 100), (r'MULTIMODAL\s*TRANSPORT', 100),
        (r'SURRENDER', 95), (r'TELEX\s*RELEASE', 95),
        (r'PORT\s*OF\s*LOADING', 60), (r'PORT\s*OF\s*DISCHARGE', 60), (r'CLEAN\s*ON\s*BOARD', 70),
        (r'FREIGHT\s*PREPAID', 60),
    ],
    'Commercial_Invoice': [
        (r'COMMERCIAL\s*INVOICE', 100), (r'TAX\s*INVOICE', 100), (r'PROFORMA\s*INVOICE', 100),
    ],
    'Packing_List': [
        (r'PACKING\s*LIST', 100), (r'DETAIL\s*OF\s*PACKING', 90),
    ],
    'Weight_List': [
        (r'WEIGHT\s*LIST', 100), (r'WEIGHT\s*CERTIFICATE', 100), (r'MEASURE\s*LIST', 90),
    ],
    'Mill_Certificate': [
        (r'MILL\s*TEST\s*CERTIFICATE', 100), (r'CHEMICAL\s*COMPOSITION', 80),
        (r'TEST\s*REPORT', 80), (r'INSPECTION\s*CERTIFICATE', 90), (r'ê²€ì‚¬\s*ì„±ì ì„œ', 100),
    ],
    'Cargo_Insurance': [
        (r'INSURANCE\s*POLICY', 100), (r'CERTIFICATE\s*OF\s*INSURANCE', 100), (r'MARINE\s*CARGO', 90),
    ],
    'Certificate_Origin': [
        (r'CERTIFICATE\s*OF\s*ORIGIN', 100), (r'COUNTRY\s*OF\s*ORIGIN', 80), (r'ì›ì‚°ì§€\s*ì¦ëª…ì„œ', 100),
    ],
    'Customs_clearance_Letter': [
        # â˜… ìš°ì„ ìˆœìœ„ ìµœê³  (í•œê¸€ ìˆ˜ì¶œì‹ ê³ í•„ì¦ - ë„ì–´ì“°ê¸° ì„ íƒì )
        (r'ìˆ˜ì¶œ\s?ì‹ ê³ \s?í•„ì¦', 105),  # ë„ì–´ì“°ê¸° ìˆê±°ë‚˜ ì—†ê±°ë‚˜
        (r'ìˆ˜ì…\s?ì‹ ê³ \s?í•„ì¦', 105),
        (r'ìˆ˜ì¶œì‹ ê³ í•„ì¦', 105),  # ë„ì–´ì“°ê¸° ì—†ëŠ” ê²½ìš° ëª…ì‹œì  ì¶”ê°€
        (r'ìˆ˜ì…ì‹ ê³ í•„ì¦', 105),
        
        # ì˜ë¬¸ íŒ¨í„´
        (r'EXPORT\s*DECLARATION', 100),
        (r'IMPORT\s*DECLARATION', 100), 
        (r'CUSTOMS\s*CLEARANCE', 100),
        
        # ì¶”ê°€ í•œê¸€ í‚¤ì›Œë“œ
        (r'ê´€ì„¸ì²­', 90),
        (r'í†µê´€ê³ ìœ ë¶€í˜¸', 90),
        (r'ìˆ˜ì¶œí†µê´€', 90),
        (r'ìˆ˜ì¶œ\s?ì‹ ê³ ', 88),  # ë” ìœ ì—°í•˜ê²Œ
        (r'ìˆ˜ì…\s?ì‹ ê³ ', 88),
        (r'EP-\d+', 95),  # Export declaration number pattern
        (r'ì‹ ê³ ë²ˆí˜¸', 85),
        (r'í†µê´€', 75),  # ë‚®ì¶¤ (ë„ˆë¬´ ì¼ë°˜ì )
    ],
    'Delivery_Note': [
        (r'DELIVERY\s*NOTE', 100), (r'DELIVERY\s*ORDER', 100), (r'ë‚©í’ˆì„œ', 100), (r'ì¸ìˆ˜ì¦', 100),
    ]
}

DOCUMENT_ID_PATTERNS = {
    'Commercial_Invoice': [r'BHS\d{10}', r'BHR\d{10}', r'HI[A-Z]\d{10}', r'Invoice\s*No\.?\s*[:\s]*([A-Z0-9-]+)'],
    'Bill_of_Lading': [r'B/L\s*No\.?\s*[:\s]*([A-Z0-9]+)', r'WYGSK[A-Z0-9]+', r'KYSC[A-Z0-9]+', r'SSSL[A-Z0-9]+', r'BJTL[A-Z0-9]+'],
    'Packing_List': [r'BHS\d{10}', r'BHR\d{10}'],
    'Customs_clearance_Letter': [r'ì‹ ê³ ë²ˆí˜¸\s*[:\s]*([0-9-]+)']
}

CONTINUATION_MARKERS = ['to be continued', 'continuation page', 'page:', 'total', 'last item', 'sub total']

try:
    pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH
except:
    print("âš ï¸ Tesseract ì„¤ì • ì˜¤ë¥˜")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ë©”ì¸ í´ë˜ìŠ¤
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class PDFSplitter:
    def __init__(self, input_path: str, output_dir: str):
        self.pdf_path = Path(input_path)
        self.output_dir = Path(output_dir)
        self.doc = None
        try:
            self.slip_no = self.pdf_path.name.split('_')[0]
        except:
            self.slip_no = "UnknownSlip"

    def __enter__(self):
        self.doc = fitz.open(self.pdf_path)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.doc:
            self.doc.close()

    def _perform_ocr(self, page, high_res=False) -> str:
        """ê°•ì œ OCR ìˆ˜í–‰ í•¨ìˆ˜ - í•œê¸€ ì¸ì‹ ìµœì í™” (ì ì‘í˜• í•´ìƒë„)"""
        try:
            # â˜… ì ì‘í˜• í•´ìƒë„ (ê¸°ë³¸ 3x3, í•„ìš”ì‹œ 4x4)
            matrix = fitz.Matrix(3, 3) if not high_res else fitz.Matrix(4, 4)
            pix = page.get_pixmap(matrix=matrix)
            img_data = pix.tobytes("png")
            image = Image.open(io.BytesIO(img_data))
            
            # í•œê¸€ ìš°ì„  ì¸ì‹ (kor+eng ìˆœì„œë¡œ ë³€ê²½)
            ocr_text = pytesseract.image_to_string(image, lang='kor+eng')
            
            # â˜… ê²°ê³¼ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ê³ í•´ìƒë„ë¡œ ì¬ì‹œë„
            if len(ocr_text.strip()) < 20 and not high_res:
                print(f"    ğŸ” Low OCR result, retrying with high resolution...")
                return self._perform_ocr(page, high_res=True)
            
            # ë””ë²„ê·¸: OCR ê²°ê³¼ ì¼ë¶€ ì¶œë ¥
            preview = ocr_text.strip()[:100] if ocr_text else "(empty)"
            res_label = "High-Res" if high_res else "Normal"
            print(f"    [OCR {res_label}] {preview}")
            
            return ocr_text
        except Exception as e:
            print(f"    [OCR Error] {str(e)}")
            return ""

    def _get_text_hybrid(self, page, page_num) -> str:
        """1ì°¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê¸€ììˆ˜ ì ìœ¼ë©´ OCR)"""
        text = page.get_text()
        if len(text.strip()) < OCR_THRESHOLD:
            print(f"  [OCR] P.{page_num+1} Scanning (Low Text)...")
            return self._perform_ocr(page)
        return text

    def _classify_page(self, text: str) -> Tuple[Optional[str], float, str]:
        text_upper = text.upper()
        best_type = None
        best_conf = 0.0
        best_method = 'unknown'
        scores = {}  # ëª¨ë“  ì ìˆ˜ ì¶”ì 

        for doc_type, patterns in DOCUMENT_PATTERNS.items():
            max_score = 0
            for pattern, score in patterns:
                if re.search(pattern, text_upper):
                    if score > max_score:
                        max_score = score
            scores[doc_type] = max_score
            
            if max_score > best_conf:
                best_conf = max_score
                best_type = doc_type
                best_method = 'header_match' if max_score >= 90 else 'content_keyword'
        
        # â˜… ì‹ ë¢°ë„ ì„ê³„ê°’ ìƒí–¥ (50 â†’ 80)
        if best_conf < 80:
            return None, 0.0, 'low_confidence'
        
        # â˜… ì ìˆ˜ ì°¨ì´ í™•ì¸ (ì• ë§¤í•œ ê²½ìš° ê°ì§€)
        sorted_scores = sorted(scores.values(), reverse=True)
        second_best = sorted_scores[1] if len(sorted_scores) > 1 else 0
        
        if best_conf - second_best < 15:  # ì ìˆ˜ ì°¨ì´ 15ì  ë¯¸ë§Œ
            print(f"    âš ï¸ Uncertain classification: {best_type}({best_conf}) vs 2nd({second_best})")
            return None, 0.0, 'uncertain'
        
        return best_type, best_conf, best_method

    def _extract_id(self, text: str, doc_type: str) -> Optional[str]:
        if not doc_type or doc_type not in DOCUMENT_ID_PATTERNS: return None
        for pattern in DOCUMENT_ID_PATTERNS[doc_type]:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                val = match.group(1) if match.groups() else match.group(0)
                return re.sub(r'[^A-Z0-9-]', '', val)
        return None


    def group_pages(self) -> List[Dict]:
        analyses = []
        for i in range(len(self.doc)):
            page = self.doc[i]
            
            # 1. ë¨¼ì € ì¼ë°˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (OCR ì—†ì´, ë¹ ë¦„)
            text = page.get_text()
            doc_type, conf, method = self._classify_page(text)
            
            # ğŸš¨ [RESCUE LOGIC] ë¶„ë¥˜ ì‹¤íŒ¨ ì‹œ OCR ì‹œë„ (ëŠë¦¬ì§€ë§Œ ì •í™•) ğŸš¨
            if doc_type is None:
                print(f"  [RESCUE] P.{i+1} Text-based classification failed. Trying OCR...")
                text = self._perform_ocr(page)  # ì´ì œ OCR ì‹¤í–‰
                doc_type, conf, method = self._classify_page(text)  # OCR í…ìŠ¤íŠ¸ë¡œ ì¬ë¶„ë¥˜
                if doc_type:
                    print(f"    âœ… Rescued! Detected: {doc_type}")
                else:
                    # ğŸ”¥ [FILENAME FALLBACK] OCRë„ ì‹¤íŒ¨ ì‹œ íŒŒì¼ëª… ê¸°ë°˜ ë¶„ë¥˜
                    filename = str(self.pdf_path.name).upper()
                    if 'EP-' in filename or 'EXPORT' in filename or 'DECLARATION' in filename:
                        doc_type = 'Customs_clearance_Letter'
                        conf = 80
                        method = 'filename_fallback'
                        print(f"    ğŸ”¥ Rescued by filename! Detected: {doc_type}")
                    else:
                        print(f"    âŒ OCR also failed. Marking as Etc.")

            doc_id = self._extract_id(text, doc_type)
            analyses.append({'page': i, 'type': doc_type, 'id': doc_id, 'conf': conf, 'method': method, 'text': text})
            
            # DEBUG: Print classification result for each page
            print(f"  ğŸ“„ Page {i+1}: Type={doc_type}, ID={doc_id}, Conf={conf}, Method={method}")

        # DEBUG: Print all analyses
        print(f"\n  [DEBUG] Total pages analyzed: {len(analyses)}")
        for idx, item in enumerate(analyses):
            print(f"    P.{idx+1}: {item['type']} | ID: {item['id']}")

        groups = []
        current = None

        for item in analyses:
            start_new = False
            if current is None:
                start_new = True
            else:
                if item['type'] is not None and item['type'] != current['type']:
                    start_new = True
                elif item['type'] is not None and item['type'] == current['type']:
                    if item['id'] and current['id'] and item['id'] != current['id']:
                        start_new = True
                    else:
                        if item['type'] in ['Weight_List', 'Packing_List', 'Mill_Certificate']:
                            start_new = False
                        else:
                            start_new = False
                elif item['type'] is None:
                    if current['type'] in ['Bill_of_Lading', 'Mill_Certificate', 'Weight_List']:
                        start_new = False
                    else:
                        start_new = False

            if start_new:
                if current: groups.append(current)
                current = {
                    'type': item['type'] or 'Etc', 'pages': [item['page']], 'id': item['id'],
                    'log': {'doc_type': item['type'], 'confidence': item['conf'], 'method': item['method']}
                }
            else:
                current['pages'].append(item['page'])
                if not current['id'] and item['id']: current['id'] = item['id']
                if current['type'] == 'Etc' and item['type']:
                    current['type'] = item['type']
                    current['log'] = {'doc_type': item['type'], 'confidence': item['conf'], 'method': item['method']}

        if current: groups.append(current)
        
        # DEBUG: Print grouping results
        print(f"\n  [DEBUG] Created {len(groups)} groups:")
        for idx, grp in enumerate(groups):
            page_range = f"P.{grp['pages'][0]+1}-{grp['pages'][-1]+1}" if len(grp['pages']) > 1 else f"P.{grp['pages'][0]+1}"
            print(f"    Group {idx+1}: {grp['type']} | {page_range} | ID: {grp['id']}")
        
        return groups

    def process(self) -> List[Dict]:
        self.output_dir.mkdir(parents=True, exist_ok=True)
        groups = self.group_pages()
        saved_files = []
        
        # Parser í´ë” ìƒì„±
        Path(PARSER_CI_INPUT).mkdir(parents=True, exist_ok=True)
        Path(PARSER_BL_INPUT).mkdir(parents=True, exist_ok=True)
        
        for grp in groups:
            doc_type = grp['type']
            pages = grp['pages']
            start, end = pages[0]+1, pages[-1]+1
            
            out_pdf = fitz.open()
            out_pdf.insert_pdf(self.doc, from_page=pages[0], to_page=pages[-1])
            
            page_str = f"{start}p" if start == end else f"{start}-{end}p"
            id_str = f"_{grp['id']}" if grp['id'] else ""
            filename = f"{self.slip_no}_{doc_type}{id_str}_{page_str}.pdf"
            if len(filename) > 100: filename = f"{self.slip_no}_{doc_type}_{page_str}.pdf"
            
            # 1. Output ì €ì¥
            save_path = self.output_dir / filename
            out_pdf.save(str(save_path))
            
            # 2. Parser í´ë” ë³µì‚¬
            if doc_type == 'Commercial_Invoice':
                out_pdf.save(str(Path(PARSER_CI_INPUT) / filename))
            elif doc_type == 'Bill_of_Lading':
                out_pdf.save(str(Path(PARSER_BL_INPUT) / filename))
            
            out_pdf.close()
            
            saved_files.append({
                "file_name": filename, "slip_no": self.slip_no, "document_type": doc_type,
                "page_range": [start, end], "document_id": grp['id'], "classification_log": grp['log']
            })
            print(f" âœ… Saved: {filename}")
            
        return saved_files

def main():
    input_path = Path(INPUT_FOLDER)
    timestamp = datetime.now().strftime("%Y%m%d%H%M")
    output_path = Path(OUTPUT_FOLDER) / timestamp
    
    if not input_path.exists(): return
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“‚ Input: {input_path}")
    print(f"ğŸ“‚ Output: {output_path}\n")
    
    all_results = []
    for pdf in list(input_path.glob("*.pdf")):
        print(f"\nğŸ“„ Processing {pdf.name}...")
        try:
            with PDFSplitter(str(pdf), str(output_path)) as splitter:
                all_results.extend(splitter.process())
        except Exception as e:
            print(f"âŒ Error: {e}")
            import traceback
            traceback.print_exc()

    with open(output_path / "processing_result.json", 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    main()
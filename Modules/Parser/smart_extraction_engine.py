# -*- coding: utf-8 -*-
"""
Smart Extraction Engine - Hybrid OCR + Gemini API
í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ PDFì—ì„œ ë°ì´í„° ì¶”ì¶œ:
1. PyMuPDFë¡œ í…ìŠ¤íŠ¸ PDF ë¹ ë¥¸ ì²˜ë¦¬
2. ìŠ¤ìº” PDFëŠ” Tesseract OCR
3. BL, Invoiceë§Œ ì„ íƒì  ì²˜ë¦¬
4. Gemini APIë¡œ í•„ë“œ ì¶”ì¶œ + ì‹ ë¢°ë„ ê³„ì‚°
"""

import fitz  # PyMuPDF
import pytesseract
from PIL import Image
import io
import os
import asyncio
import json
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import google.generativeai as genai

# Tesseract ê²½ë¡œ ì„¤ì •
TESSERACT_PATH = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
try:
    pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH
except:
    print("âš ï¸ Tesseract ê²½ë¡œ ì„¤ì • ì˜¤ë¥˜")

# OCR ì„ê³„ê°’
OCR_THRESHOLD = 100  # í…ìŠ¤íŠ¸ê°€ ì´ ê°’ë³´ë‹¤ ì ìœ¼ë©´ ìŠ¤ìº” PDFë¡œ ê°„ì£¼

# ë¬¸ì„œ íƒ€ì… ê²€ì¶œ íŒ¨í„´ (pdf_splitter.pyì—ì„œ ê°€ì ¸ì˜´)
DOCUMENT_PATTERNS = {
    'BL': [  # Bill of Lading
        (r'BILL\s*OF\s*LADING', 100),
        (r'WAYBILL', 100),
        (r'MULTIMODAL\s*TRANSPORT', 100),
        (r'SURRENDER', 95),
        (r'TELEX\s*RELEASE', 95),
        (r'PORT\s*OF\s*LOADING', 60),
        (r'PORT\s*OF\s*DISCHARGE', 60),
        (r'CLEAN\s*ON\s*BOARD', 70),
        (r'FREIGHT\s*PREPAID', 60),
    ],
    'INVOICE': [  # Commercial Invoice
        (r'COMMERCIAL\s*INVOICE', 100),
        (r'PROFORMA\s*INVOICE', 100),
        (r'INVOICE\s*NO', 80),
    ],
    'PACKING_LIST': [  # Packing List (í•„í„°ë§ìš©)
        (r'PACKING\s*LIST', 100),
        (r'DETAIL\s*OF\s*PACKING', 90),
    ],
}


class SmartExtractionEngine:
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.gemini_model = None
        self.extraction_config = None
        self.data_normalizer = None
        self.semaphore = asyncio.Semaphore(5)  # âœ… ë³‘ë ¬ ì²˜ë¦¬ ì œí•œ (Semaphore) ì¶”ê°€
        self._load_configs()
    
    def _load_configs(self):
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
            import sys
            from pathlib import Path
            project_root = Path(__file__).parent.parent.parent
            if str(project_root) not in sys.path:
                sys.path.insert(0, str(project_root))
            
            # Gemini API í‚¤ ë¡œë“œ
            from Config.api_config import GEMINI_API_KEY
            genai.configure(api_key=GEMINI_API_KEY)
            
            # ìµœì‹  ì•ˆì • ëª¨ë¸ (ì‚¬ìš©ì í™˜ê²½ì—ì„œ 2.0 ì´ìƒ ì§€ì› í™•ì¸ë¨)
            GEMINI_MODEL_NAME = 'models/gemini-2.0-flash'
            
            # ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ (ì‚¬ìš©ì ì œì•ˆ ë°˜ì˜)
            SYSTEM_INSTRUCTION = """
            ë‹¹ì‹ ì€ ë¬¼ë¥˜ ë¬¸ì„œ(B/L, Invoice) ì „ë¬¸ ë°ì´í„° ì¶”ì¶œ ì—”ì§„ì…ë‹ˆë‹¤. 
            JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë©°, ë‚ ì§œ í˜•ì‹(YYYY-MM-DD), í†µí™” ì½”ë“œ ì¤€ìˆ˜ ë“± ë°ì´í„° ì •ê·œí™” ê·œì¹™ì„ ì—„ê²©íˆ ë”°ë¦…ë‹ˆë‹¤.
            ìˆ˜ëŸ‰ê³¼ ê¸ˆì•¡ì€ ë°˜ë“œì‹œ ìˆ«ì í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ê³ , ì‰¼í‘œ ë“±ì€ ì œê±°í•˜ì„¸ìš”.
            """
            
            try:
                self.gemini_model = genai.GenerativeModel(
                    model_name=GEMINI_MODEL_NAME,
                    system_instruction=SYSTEM_INSTRUCTION
                )
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
                test_response = self.gemini_model.generate_content("Hello")
                if test_response and test_response.text:
                    print(f"âœ… Gemini API ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {GEMINI_MODEL_NAME})")
                else:
                    raise Exception("í…ŒìŠ¤íŠ¸ ì‘ë‹µ ì—†ìŒ")
            except Exception as e:
                print(f"âš ï¸ {GEMINI_MODEL_NAME} ì‹¤íŒ¨: {e}")
                # ëŒ€ì²´ ëª¨ë¸ ëª©ë¡ (ìµœì‹  ë²„ì „ ìš°ì„  ìˆœìœ„)
                fallback_models = [
                    'models/gemini-2.5-flash',       # ìµœì‹  2.5 ë²„ì „
                    'models/gemini-2.5-flash-lite',  # 2.5 ê²½ëŸ‰ ë²„ì „
                    'models/gemini-3-flash',         # ì°¨ì„¸ëŒ€ 3 ë²„ì „
                    'models/gemini-1.5-flash',       # ê¸°ì¡´ ì•ˆì • ë²„ì „
                    'models/gemini-1.5-pro'          # ê³ ì„±ëŠ¥ (ë¹„ìš©â†‘)
                ]
                model_loaded = False
                
                for fallback_model in fallback_models:
                    try:
                        print(f"   ì‹œë„ ì¤‘: {fallback_model}")
                        self.gemini_model = genai.GenerativeModel(
                            model_name=fallback_model,
                            system_instruction=SYSTEM_INSTRUCTION
                        )
                        test_response = self.gemini_model.generate_content("Hello")
                        if test_response and test_response.text:
                            print(f"âœ… Gemini API ì´ˆê¸°í™” ì™„ë£Œ (ëŒ€ì²´ ëª¨ë¸: {fallback_model})")
                            model_loaded = True
                            break
                    except Exception as fb_error:
                        print(f"   âŒ {fallback_model} ì‹¤íŒ¨: {fb_error}")
                        continue
                
                if not model_loaded:
                    print("\nğŸ’¡ í•´ê²° ë°©ë²•:")
                    print("   1. Google Cloud Consoleì—ì„œ 'Generative Language API' í™œì„±í™”")
                    print("   2. https://console.cloud.google.com/apis/library")
                    print("   3. API í‚¤ í™•ì¸: Config/api_config.py")
                    raise Exception("ëª¨ë“  Gemini ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        except ImportError:
            print("âŒ Config/api_config.py íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. api_config.example.pyë¥¼ ë³µì‚¬í•˜ì—¬ API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
            raise
        except Exception as e:
            print(f"âŒ Gemini API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
        
        # ì¶”ì¶œ í•„ë“œ ì„¤ì • ë¡œë“œ
        try:
            config_path = Path(__file__).parent.parent.parent / 'Config' / 'extraction_config.json'
            with open(config_path, 'r', encoding='utf-8') as f:
                self.extraction_config = json.load(f)
            print("âœ… Extraction Config ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ extraction_config.json ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.extraction_config = {"default_fields": [], "optional_fields": [], "document_types": {}}
        
        # DataNormalizer ì´ˆê¸°í™”
        try:
            from backend.logic.data_normalizer import DataNormalizer
            self.data_normalizer = DataNormalizer()
            print("âœ… DataNormalizer ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ DataNormalizer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.data_normalizer = None

        # âœ… TextPreprocessor ì´ˆê¸°í™”
        try:
            from Modules.Parser.text_preprocessor import TextPreprocessor
            self.text_preprocessor = TextPreprocessor()
            print("âœ… TextPreprocessor ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ TextPreprocessor ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.text_preprocessor = None
            
        # âœ… CombinationFinder ì´ˆê¸°í™”
        try:
            from Modules.Parser.combination_finder import CombinationFinder
            self.combination_finder = CombinationFinder()
            print("âœ… CombinationFinder ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸ CombinationFinder ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.combination_finder = None
    
    
    def detect_document_type(self, pdf_path: str, page_num: int) -> Tuple[str, str]:
        """
        ë¬¸ì„œ íƒ€ì… ê°ì§€ (íŒŒì¼ëª… ì ˆëŒ€ ìš°ì„ )
        """
        filename = os.path.basename(pdf_path).upper()
        
        # 1. íŒŒì¼ëª… ê¸°ë°˜ ë¶„ë¥˜ (ê°€ì¥ ê°•ë ¥í•œ ê·œì¹™)
        if "BILL_OF_LADING" in filename or "WAYBILL" in filename:
            print(f"   ğŸ¯ íŒŒì¼ëª… ê·œì¹™ ì ìš©: BL ({filename})")
            doc = fitz.open(pdf_path)
            page = doc[page_num]
            text = page.get_text()
            doc.close()
            return "BL", text
            
        if "COMMERCIAL_INVOICE" in filename or "INVOICE" in filename:
            # PACKING LISTëŠ” ì œì™¸
            if "PACKING" not in filename:
                print(f"   ğŸ¯ íŒŒì¼ëª… ê·œì¹™ ì ìš©: INVOICE ({filename})")
                doc = fitz.open(pdf_path)
                page = doc[page_num]
                text = page.get_text()
                doc.close()
                return "INVOICE", text
        
        # 2. íŒŒì¼ëª…ìœ¼ë¡œ ì‹ë³„ ë¶ˆê°€í•œ ê²½ìš°ì—ë§Œ ë‚´ìš© ë¶„ì„
        print(f"   âš ï¸ íŒŒì¼ëª… ê·œì¹™ ì‹¤íŒ¨, ë‚´ìš© ë¶„ì„ ì‹œë„: {filename}")
        try:
            doc = fitz.open(pdf_path)
            page = doc[page_num]
            text = page.get_text()
            doc.close()
            
            # í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ìœ¼ë©´ ìŠ¤ìº” ë¬¸ì„œ
            if len(text.strip()) < OCR_THRESHOLD:
                return "SCAN_REQUIRED", text
            
            # Regex íŒ¨í„´ìœ¼ë¡œ ë¬¸ì„œ íƒ€ì… ê²€ì¶œ (ì ìˆ˜ ê¸°ë°˜)
            text_upper = text.upper()
            best_type = None
            best_conf = 0.0
            
            for doc_type, patterns in DOCUMENT_PATTERNS.items():
                for pattern, score in patterns:
                    if re.search(pattern, text_upper):
                        if score > best_conf:
                            best_conf = score
                            best_type = doc_type
            
            # ì‹ ë¢°ë„ê°€ 50 ë¯¸ë§Œì´ë©´ UNKNOWN
            if best_conf < 50:
                return "UNKNOWN", text
            
            return best_type, text
                
        except Exception as e:
            print(f"âš ï¸ ë¬¸ì„œ íƒ€ì… ê²€ì¶œ ì˜¤ë¥˜: {e}")
            return "UNKNOWN", ""
    
    

    def high_quality_ocr(self, pdf_path: str, page_num: int) -> str:
        """
        ê³ í’ˆì§ˆ OCR (ì „ì²´ í˜ì´ì§€) - ìŠ¤ìº” ë¬¸ì„œìš©
        """
        try:
            doc = fitz.open(pdf_path)
            page = doc[page_num]
            
            # ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„± (300 DPI ì´ìƒ ê¶Œì¥, zoom=2.0)
            pix = page.get_pixmap(matrix=fitz.Matrix(3.0, 3.0))
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            doc.close()
            
            # ì „ì²´ OCR ì‹¤í–‰ (í•œê¸€+ì˜ì–´)
            text = pytesseract.image_to_string(img, lang='eng+kor')
            return text
            
        except Exception as e:
            print(f"âŒ ê³ í’ˆì§ˆ OCR ì˜¤ë¥˜: {e}")
            return ""

    async def extract_with_gemini_async(self, ocr_text: str, doc_type: str, extraction_fields: List[Dict], expected_values: Dict = None) -> Dict:
        """
        Gemini APIë¡œ í•„ë“œ ì¶”ì¶œ (ë¹„ë™ê¸°) with êµ¬ì¡°í™”ëœ ìš”ì²­
        """
        if not self.gemini_model:
            raise Exception("Gemini APIê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # ë¬¸ì„œ íƒ€ì…ë³„ ì§€ì‹œì‚¬í•­
        doc_instruction = ""
        if doc_type == "BL":
            doc_instruction = "ì´ ë¬¸ì„œëŠ” ì„ í•˜ì¦ê¶Œ(Bill of Lading)ì…ë‹ˆë‹¤. ì„ ë°• ì •ë³´, ìš´ì†¡ ì •ë³´, Incoterms, ê·¸ë¦¬ê³  **ì¤‘ëŸ‰ ì •ë³´(Net Weight, Gross Weight)**ë¥¼ ì£¼ì˜ ê¹Šê²Œ ì°¾ì•„ ì¶”ì¶œí•˜ì„¸ìš”."
        elif doc_type == "INVOICE":
            doc_instruction = "ì´ ë¬¸ì„œëŠ” ìƒì—…ì†¡ì¥(Commercial Invoice)ì…ë‹ˆë‹¤. ê¸ˆì•¡ê³¼ ê±°ë˜ ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”."
        
        # ê¸°ëŒ€ ê°’(Expected Values) íŒíŠ¸ ì¶”ê°€
        hint_instruction = ""
        if expected_values:
            hint_instruction = "\n**[ì¤‘ìš” íŒíŠ¸]** ë‹¤ìŒì€ ì´ ë¬¸ì„œì—ì„œ ê¸°ëŒ€ë˜ëŠ” ê°’ë“¤ì…ë‹ˆë‹¤. ë¬¸ì„œ ë‚´ì—ì„œ ì´ ê°’ë“¤ê³¼ ì¼ì¹˜í•˜ëŠ” í•­ëª©ì„ ìš°ì„ ì ìœ¼ë¡œ ì°¾ì•„ë³´ì„¸ìš”:\n"
            if 'total_amount' in expected_values:
                hint_instruction += f"- ê¸°ëŒ€ ì´ ê¸ˆì•¡: {expected_values['total_amount']}\n"
            if 'total_quantity' in expected_values:
                hint_instruction += f"- ê¸°ëŒ€ ì´ ìˆ˜ëŸ‰: {expected_values['total_quantity']}\n"
            hint_instruction += "ë§Œì•½ ê¸°ëŒ€ ê°’ê³¼ ë¬¸ì„œìƒì˜ ê°’ì´ ë‹¤ë¥´ë‹¤ë©´, ë¬¸ì„œìƒì˜ ê°’ì„ ì¶”ì¶œí•˜ê³  ê·¸ ì´ìœ ë¥¼ 'notes' í•„ë“œì— ê¸°ë¡í•˜ì„¸ìš”.\n"

        # í•„ë“œë³„ í”„ë¡¬í”„íŠ¸ ë° ì˜ˆìƒ ì‘ë‹µ êµ¬ì¡° ìƒì„±
        field_prompts = ""
        expected_fields_example = {}
        
        for field in extraction_fields:
            output_format = field.get('output_format', {})
            field_prompt = field['prompt']
            
            # Incoterms í•„ë“œì— ëŒ€í•œ ì¶”ê°€ íŒíŠ¸
            if 'incoterms' in field['name'].lower():
                field_prompt += " (ì˜ˆ: FOB, CIF, EXW, DDP ë“±)"
            
            field_prompts += f"- {field['label']} ({field['name']}): {field_prompt}\n"
            
            # ì˜ˆìƒ ì‘ë‹µ í˜•íƒœ ìƒì„±
            if 'currency' in output_format:
                expected_fields_example[field['name']] = {"value": 0.0, "currency": "USD", "coordinates": [0, 0, 0, 0]}
            elif 'unit' in output_format:
                expected_fields_example[field['name']] = {"value": 0.0, "unit": "MT", "coordinates": [0, 0, 0, 0]}
            elif output_format.get('format') == 'date':
                expected_fields_example[field['name']] = {"value": "YYYY-MM-DD", "format": "date", "coordinates": [0, 0, 0, 0]}
            else:
                expected_fields_example[field['name']] = {"value": "ì¶”ì¶œëœ ê°’", "format": "text", "coordinates": [0, 0, 0, 0]}
        
        # âœ… Text Preprocessing (Token Optimization)
        if self.text_preprocessor:
            print(f"   ğŸ§¹ Preprocessing text... (Original: {len(ocr_text)} chars)")
            ocr_text = self.text_preprocessor.preprocess(ocr_text, doc_type)
            print(f"   âœ¨ Preprocessed: {len(ocr_text)} chars")

        # âœ… N:1 Combination Finder (Python Logic)
        combo_hint = ""
        if self.combination_finder and expected_values:
            # 1. Check Amount
            exp_amount = expected_values.get('total_amount')
            if exp_amount:
                try:
                    # Remove commas if string
                    target_amt = float(str(exp_amount).replace(',', ''))
                    combo = self.combination_finder.find_combination(ocr_text, target_amt)
                    if combo:
                        combo_str = " + ".join([f"{n:,.2f}" for n in combo])
                        combo_hint += f"- ğŸ’¡ Found combination for Amount: {combo_str} = {target_amt:,.2f}\n"
                except:
                    pass

            # 2. Check Quantity
            exp_qty = expected_values.get('total_quantity')
            if exp_qty:
                try:
                    target_qty = float(str(exp_qty).replace(',', ''))
                    combo = self.combination_finder.find_combination(ocr_text, target_qty)
                    if combo:
                        combo_str = " + ".join([f"{n:,.2f}" for n in combo])
                        combo_hint += f"- ğŸ’¡ Found combination for Quantity: {combo_str} = {target_qty:,.2f}\n"
                except:
                    pass
        
        if combo_hint:
            hint_instruction += "\n**[N:1 MATCHING HINTS]**\n" + combo_hint
            hint_instruction += "Use these combinations to verify the total amount/quantity. If they match, extract the individual items as evidence.\n"

        prompt = f"""
ë‹¤ìŒ {doc_type} ë¬¸ì„œì˜ OCR í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì„¸ìš”:

{doc_instruction}
{hint_instruction}

{ocr_text}

**ì¶”ì¶œí•  í•„ë“œ:**
{field_prompts}

**ì‘ë‹µ í˜•ì‹ (JSON only):**
{{
  "document_type": "{doc_type}",
  "confidence": 0.95,
  "fields": {json.dumps(expected_fields_example, ensure_ascii=False, indent=2)},
  "field_confidence": {{
    {', '.join([f'"{f["name"]}": 0.95' for f in extraction_fields])}
  }},
  "evidence": [
    {{
      "field": "total_amount",
      "values": [100.00, 200.00],
      "coordinates": [[ymin, xmin, ymax, xmax], [ymin, xmin, ymax, xmax]],
      "reason": "Sum of line items matches expected total"
    }}
  ],
  "notes": "ì¶”ì¶œ ê³¼ì •ì—ì„œ íŠ¹ì´ì‚¬í•­ì´ë‚˜ ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„"
}}
"""
        
        # â˜… FIX: ì¬ì‹œë„ íšŸìˆ˜ ì¶•ì†Œ (ì‚¬ìš©ì ìš”ì²­: 3 -> 1)
        max_retries = 1
        retry_delay = 2
        
        for attempt in range(max_retries + 1):
            try:
                # ë¹„ë™ê¸° í˜¸ì¶œ (JSON ëª¨ë“œ ì ìš©)
                response = await self.gemini_model.generate_content_async(
                    prompt,
                    generation_config={"response_mime_type": "application/json"}
                )
                
                # í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚°
                usage = response.usage_metadata
                input_tokens = usage.prompt_token_count
                output_tokens = usage.candidates_token_count
                total_tokens = usage.total_token_count
                
                # ì˜ˆìƒ ë¹„ìš© ê³„ì‚° (Gemini 1.5 Flash ê¸°ì¤€)
                input_cost = (input_tokens / 1_000_000) * 0.075 * 1400
                output_cost = (output_tokens / 1_000_000) * 0.30 * 1400
                total_cost = input_cost + output_cost
                
                token_info = {
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                    "total_tokens": total_tokens,
                    "estimated_cost_krw": round(total_cost, 2)
                }
                
                # JSON ëª¨ë“œì´ë¯€ë¡œ ë°”ë¡œ íŒŒì‹± ê°€ëŠ¥
                raw_result = json.loads(response.text)
                raw_result['token_usage'] = token_info
                
                # âœ… DataNormalizer ì ìš©
                if self.data_normalizer:
                    normalized_result = self.data_normalizer.normalize_extraction_result(raw_result)
                    normalized_result['token_usage'] = token_info
                else:
                    normalized_result = raw_result
                
                # âœ… ê²°ê³¼ ê²€ì¦ (Validation)
                is_valid = True
                validation_note = ""
                
                critical_fields = []
                if doc_type == "INVOICE":
                    critical_fields = ['total_amount']
                elif doc_type == "BL":
                    critical_fields = ['bl_number']
                
                fields = normalized_result.get('fields', {})
                
                for field_name in critical_fields:
                    field_data = fields.get(field_name, {})
                    if not field_data or field_data.get('value') in [None, "", 0]:
                        is_valid = False
                        validation_note = f"Critical field missing: {field_name}"
                        break
                
                # âœ… N:1 ë§¤ì¹­ ê²€ì¦ ì¶”ê°€ (Amount/Quantity Mismatch)
                if is_valid and doc_type == "INVOICE" and expected_values:
                    def normalize_val(v):
                        if v is None: return None
                        if isinstance(v, (int, float)): return float(v)
                        try:
                            # ì½¤ë§ˆ ì œê±° í›„ float ë³€í™˜
                            return float(str(v).replace(',', '').strip())
                        except:
                            return None

                    ext_amount = normalize_val(fields.get('total_amount', {}).get('value'))
                    ext_qty = normalize_val(fields.get('total_quantity', {}).get('value'))
                    
                    exp_amount = normalize_val(expected_values.get('total_amount'))
                    exp_qty = normalize_val(expected_values.get('total_quantity'))
                    
                    # ê¸°ëŒ€ê°’ì´ ìˆê³  ì¶”ì¶œê°’ì´ ë‹¤ë¥¼ ê²½ìš° (N:1 ìƒí™© ì˜ì‹¬)
                    if (exp_amount is not None and ext_amount != exp_amount) or \
                       (exp_qty is not None and ext_qty != exp_qty):
                        is_valid = False
                        validation_note = f"N:1 Mismatch (Ext: {ext_amount}/{ext_qty}, Exp: {exp_amount}/{exp_qty})"
                        
                        # ë‹¤ìŒ ì¬ì‹œë„ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ë³´ê°• (ë¼ì¸ ì•„ì´í…œ ì •ë°€ ë¶„ì„ ì§€ì‹œ)
                        prompt += f"""
                            **OUTPUT REQUIREMENT:**
                            - **CRITICAL**: Set the `total_amount` field to the **SUMMED VALUE** ({exp_amount}), NOT the document total.
                            - In the 'evidence' field, you MUST explain the combination: e.g., "Sum of Line Item 1 (100.00) and Line Item 3 (200.00) matches expected {exp_amount}".
                            - In the 'notes' field, state clearly: "Found via N:1 combination".
                            """
                
                if is_valid or len(ocr_text) < 50:
                    return normalized_result
                else:
                    print(f"   âš ï¸ ê²€ì¦ ì‹¤íŒ¨ ({attempt+1}/{max_retries}): {validation_note}")
                    if attempt < max_retries:
                        await asyncio.sleep(retry_delay * (2 ** attempt))
                        continue
                    else:
                        normalized_result['notes'] = f"{normalized_result.get('notes', '')} [Validation Failed: {validation_note}]"
                        return normalized_result
                
            except Exception as e:
                error_msg = str(e)
                print(f"âŒ Gemini API ì˜¤ë¥˜ ({attempt+1}/{max_retries}): {error_msg}")
                
                # 429 ì˜¤ë¥˜ ì²˜ë¦¬ (í• ë‹¹ëŸ‰ ì´ˆê³¼)
                if "429" in error_msg or "quota" in error_msg.lower():
                    # ì¼ì¼ í• ë‹¹ëŸ‰ ì†Œì§„ ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨
                    if "daily" in error_msg.lower():
                        print("ğŸš¨ ì¼ì¼ í• ë‹¹ëŸ‰ì´ ëª¨ë‘ ì†Œì§„ë˜ì—ˆìŠµë‹ˆë‹¤. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                        raise Exception("Gemini API ì¼ì¼ í• ë‹¹ëŸ‰ ì´ˆê³¼")
                    
                    # ì¬ì‹œë„ ëŒ€ê¸° ì‹œê°„ íŒŒì‹± (ì˜ˆ: "Wait 49s")
                    wait_match = re.search(r'wait\s*(\d+)s', error_msg.lower())
                    wait_time = int(wait_match.group(1)) if wait_match else 60
                    
                    print(f"   â³ í• ë‹¹ëŸ‰ ì´ˆê³¼. {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    await asyncio.sleep(wait_time)
                    continue

                if attempt < max_retries:
                    await asyncio.sleep(retry_delay * (2 ** attempt))
                else:
                    return {
                        "document_type": doc_type,
                        "confidence": 0.0,
                        "fields": {},
                        "field_confidence": {},
                        "notes": f"API ì˜¤ë¥˜ (Max Retries): {e}"
                    }

    def _generate_search_variations(self, text: str) -> List[str]:
        """Generate variations of text for robust searching (numbers, dates)"""
        variations = [str(text)]
        text_str = str(text).strip()
        
        # 1. Handle Numbers (remove/add commas, spaces)
        clean_text = text_str.replace(',', '').replace(' ', '').strip()
        if clean_text:
            variations.append(clean_text)
            try:
                # Try to format as number with commas
                float_val = float(clean_text)
                variations.extend([
                    f"{float_val:,.2f}",  # 1,234.56
                    f"{float_val:,.0f}",  # 1,234
                    f"{float_val:.2f}",   # 1234.56 (no comma)
                    f"{float_val:.0f}",   # 1234 (no comma)
                ])
                if float_val.is_integer():
                    variations.append(f"{int(float_val)}")  # 1234
                    variations.append(f"{int(float_val):,}") # 1,234
            except:
                pass

        # 2. Handle Dates (YYYY-MM-DD -> various formats)
        # ISO format
        if re.match(r'^\d{4}-\d{2}-\d{2}$', text_str):
            try:
                from datetime import datetime
                dt = datetime.strptime(text_str, "%Y-%m-%d")
                variations.extend([
                    # Common international formats
                    dt.strftime("%d-%b-%y"),      # 24-Dec-24
                    dt.strftime("%d-%b-%Y"),      # 24-Dec-2024
                    dt.strftime("%b %d, %Y"),     # Dec 24, 2024
                    dt.strftime("%d %b %Y"),      # 24 Dec 2024
                    dt.strftime("%Y. %m. %d"),    # 2024. 12. 24
                    dt.strftime("%Y/%m/%d"),      # 2024/12/24
                    dt.strftime("%d/%m/%Y"),      # 24/12/2024
                    dt.strftime("%m/%d/%Y"),      # 12/24/2024
                    # With uppercase month
                    dt.strftime("%d-%b-%Y").upper(),  # 24-DEC-2024
                    dt.strftime("%d %b %Y").upper(),  # 24 DEC 2024
                    # Without leading zeros
                    f"{dt.day}-{dt.strftime('%b')}-{dt.year}",  # 8-Dec-2024
                    f"{dt.day} {dt.strftime('%b')} {dt.year}",  # 8 Dec 2024
                    # Dot separators
                    dt.strftime("%d.%m.%Y"),      # 24.12.2024
                    # Space variations
                    dt.strftime("%Y %m %d"),      # 2024 12 24
                ])
            except:
                pass
                
        # Remove duplicates and empty strings
        return list(set([v for v in variations if v]))

    def _find_text_coordinates(self, pdf_path: str, text_to_find: str) -> List[int]:
        """
        PDFì—ì„œ í…ìŠ¤íŠ¸ ì¢Œí‘œ ì°¾ê¸° (Robust)
        Returns: [ymin, xmin, ymax, xmax] (0~1000 normalized)
        """
        if not text_to_find:
            return [0, 0, 0, 0]
            
        try:
            variations = self._generate_search_variations(text_to_find)
            
            doc = fitz.open(pdf_path)
            # ì²« 3í˜ì´ì§€ë§Œ ê²€ìƒ‰
            for i in range(min(len(doc), 3)):
                page = doc[i]
                width = page.rect.width
                height = page.rect.height
                
                for variant in variations:
                    rects = page.search_for(variant)
                    if rects:
                        rect = rects[0]
                        xmin = int((rect.x0 / width) * 1000)
                        ymin = int((rect.y0 / height) * 1000)
                        xmax = int((rect.x1 / width) * 1000)
                        ymax = int((rect.y1 / height) * 1000)
                        doc.close()
                        return [ymin, xmin, ymax, xmax]
            
            doc.close()
            return [0, 0, 0, 0]
            
        except Exception as e:
            print(f"âš ï¸ ì¢Œí‘œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return [0, 0, 0, 0]

    def _find_multiple_text_coordinates(self, pdf_path: str, text_list: List[str]) -> List[List[int]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ ì¢Œí‘œë¥¼ í•œ ë²ˆì— ê²€ìƒ‰ (ìµœì í™”)
        """
        if not text_list:
            return []
        
        results = []
        try:
            doc = fitz.open(pdf_path)
            
            for text_to_find in text_list:
                found = False
                variations = self._generate_search_variations(text_to_find)
                
                for i in range(min(len(doc), 3)):
                    page = doc[i]
                    width = page.rect.width
                    height = page.rect.height
                    
                    for variant in variations:
                        rects = page.search_for(variant)
                        if rects:
                            rect = rects[0]
                            xmin = int((rect.x0 / width) * 1000)
                            ymin = int((rect.y0 / height) * 1000)
                            xmax = int((rect.x1 / width) * 1000)
                            ymax = int((rect.y1 / height) * 1000)
                            results.append([ymin, xmin, ymax, xmax])
                            found = True
                            break
                    if found: break
                
                if not found:
                    results.append([0, 0, 0, 0])
            
            doc.close()
            return results
        except Exception as e:
            print(f"âš ï¸ ë‹¤ì¤‘ ì¢Œí‘œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return [[0, 0, 0, 0]] * len(text_list)

    async def process_single_pdf_async(self, pdf_path: str, slip_id: str, extraction_mode: str = 'basic', expected_values: Dict = None) -> Dict:
        """
        ë‹¨ì¼ PDF íŒŒì¼ ë¹„ë™ê¸° ì²˜ë¦¬
        """
        filename = os.path.basename(pdf_path).upper()
        
        # 0. íŒŒì¼ëª… ê¸°ë°˜ 1ì°¨ í•„í„°ë§
        is_target = False
        if "BILL_OF_LADING" in filename or "WAYBILL" in filename:
            is_target = True
        elif ("COMMERCIAL_INVOICE" in filename or "INVOICE" in filename) and "PACKING" not in filename:
            is_target = True
            
        if not is_target:
            return {
                "slip_id": slip_id,
                "documents": [],
                "source": "pdf_ocr"
            }

        print(f"ğŸ“„ ì²˜ë¦¬ ì‹œì‘: {os.path.basename(pdf_path)}")
        
        # ë¬¸ì„œ íƒ€ì… ê²°ì • (íŒŒì¼ëª… ê¸°ë°˜)
        doc_type = "UNKNOWN"
        if "BILL_OF_LADING" in filename or "WAYBILL" in filename:
            doc_type = "BL"
        elif "COMMERCIAL_INVOICE" in filename or "INVOICE" in filename:
            doc_type = "INVOICE"
        
        print(f"   âœ… íƒ€ì… í™•ì •: {os.path.basename(pdf_path)} -> {doc_type}")
        
        # âœ… extraction_modeì— ë”°ë¼ í•„ë“œ ì„ íƒ
        extraction_modes = self.extraction_config.get('extraction_modes', {})
        mode_config = extraction_modes.get(extraction_mode, extraction_modes.get('basic'))  # fallback to basic
        
        if mode_config and 'document_types' in mode_config:
            document_types = mode_config['document_types']
            if doc_type in document_types:
                extraction_fields = document_types[doc_type].get('fields', [])
                print(f"   ğŸ“‹ {extraction_mode} ëª¨ë“œ - {doc_type} í•„ë“œ: {len(extraction_fields)}ê°œ")
            else:
                print(f"   âš ï¸ {doc_type} ì„¤ì • ì—†ìŒ, ìŠ¤í‚µ")
                return {
                    "slip_id": slip_id,
                    "documents": [],
                    "source": "pdf_ocr"
                }
        else:
            print(f"   âš ï¸ extraction_mode '{extraction_mode}' ì„¤ì • ì—†ìŒ")
            return {
                "slip_id": slip_id,
                "documents": [],
                "source": "pdf_ocr"
            }
        
        documents = []
        
        try:
            full_text = ""
            doc = fitz.open(pdf_path)
            num_pages = len(doc)
            pages_to_read = min(num_pages, 3)
            
            for i in range(pages_to_read):
                page = doc[i]
                text = page.get_text()
                if len(text.strip()) < OCR_THRESHOLD:
                    text = self.high_quality_ocr(pdf_path, i)
                full_text += f"\n\n{text}"
            
            doc.close()
            
            # 3. Gemini API ë¹„ë™ê¸° í˜¸ì¶œ
            print(f"   ğŸ¤– API ìš”ì²­: {os.path.basename(pdf_path)}")
            extraction_result = await self.extract_with_gemini_async(full_text, doc_type, extraction_fields, expected_values)
            
            # âœ… Post-processing: ì¢Œí‘œ ì°¾ê¸° (Highlighting)
            print(f"   ğŸ” ì¢Œí‘œ ê²€ìƒ‰ ì¤‘: {os.path.basename(pdf_path)}")
            fields = extraction_result.get('fields', {})
            for field_name, field_data in fields.items():
                if isinstance(field_data, dict):
                    value = field_data.get('value')
                    # ê°’ì´ ìˆê³  ì¢Œí‘œê°€ ì—†ê±°ë‚˜ [0,0,0,0]ì¸ ê²½ìš° ê²€ìƒ‰
                    if value and (not field_data.get('coordinates') or field_data.get('coordinates') == [0, 0, 0, 0]):
                        coords = self._find_text_coordinates(pdf_path, str(value))
                        field_data['coordinates'] = coords
            
            # Evidence ì¢Œí‘œ ì°¾ê¸° (N:1)
            evidence = extraction_result.get('evidence')
            if evidence and isinstance(evidence, list):
                print(f"   ğŸ” Evidence ì¢Œí‘œ ê²€ìƒ‰ ì¤‘ ({len(evidence)} items)")
                for item in evidence:
                    # AIê°€ ì¢Œí‘œë¥¼ ì£¼ì§€ ì•Šì•˜ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš°
                    if not item.get('coordinates') or not any(item.get('coordinates', [])):
                        values = item.get('values', [])
                        
                        if values:
                            # Optimized: Find all coordinates at once
                            found_coords = self._find_multiple_text_coordinates(pdf_path, [str(v) for v in values])
                            
                            # Filter out [0,0,0,0]
                            valid_coords = [c for c in found_coords if c != [0, 0, 0, 0]]
                            
                            if valid_coords:
                                item['coordinates'] = valid_coords
                                print(f"     - Evidence coords found: {len(valid_coords)} boxes")
                            else:
                                item['coordinates'] = []
                        else:
                            item['coordinates'] = []

            extraction_result['page'] = 1
            extraction_result['type'] = doc_type
            extraction_result['file_name'] = os.path.basename(pdf_path)
            
            documents.append(extraction_result)
            print(f"   âœ¨ ì™„ë£Œ: {os.path.basename(pdf_path)}")
            
            return {
                "slip_id": slip_id,
                "documents": documents,
                "source": "pdf_ocr"
            }
            
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ({os.path.basename(pdf_path)}): {e}")
            return {
                "slip_id": slip_id,
                "documents": [],
                "error": str(e)
            }

    async def process_project_pdfs_async(self, project_id: str, split_dir: str, extraction_mode: str = 'basic', target_ids: List[str] = None, progress_callback=None, expected_values_map: Dict = None) -> List[Dict]:
        """
        í”„ë¡œì íŠ¸ ì „ì²´ PDF ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬
        OPTIMIZED: Only scans extraction_targets/ subfolder (BL & Invoice)
        Falls back to root folder for backward compatibility
        
        Args:
            project_id: í”„ë¡œì íŠ¸ ID
            split_dir: split_documents í´ë” ê²½ë¡œ
            extraction_mode: 'basic' ë˜ëŠ” 'detailed'
            target_ids: ì²˜ë¦¬í•  ì „í‘œ ID ë¦¬ìŠ¤íŠ¸ (Noneì´ë©´ ì „ì²´ ì²˜ë¦¬)
            progress_callback: ì§„í–‰ë¥  ì½œë°± í•¨ìˆ˜ (current, total, doc_number, message)
            expected_values_map: ì „í‘œë³„ ê¸°ëŒ€ ê¸ˆì•¡/ìˆ˜ëŸ‰ íŒíŠ¸ ë§µ {slip_id: {total_amount: X, total_quantity: Y}}
        """
        from datetime import datetime
        
        print(f"ğŸš€ í”„ë¡œì íŠ¸ {project_id} ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œì‘ (ëª¨ë“œ: {extraction_mode}, ëŒ€ìƒ: {'ì „ì²´' if not target_ids else len(target_ids)})")
        slip_folders = [f for f in os.listdir(split_dir) if os.path.isdir(os.path.join(split_dir, f))]
        total_slips = len([s for s in slip_folders if target_ids is None or s in target_ids])
        current_slip = 0
        
        if progress_callback:
            progress_callback(0, total_slips, "", "ì¶”ì¶œ ì‹œì‘...")

        # âœ… ì„¸ë§ˆí¬ì–´ë¥¼ ì´ìš©í•œ ë¹„ë™ê¸° ì‘ì—… ì •ì˜
        async def sem_task(pdf_path, slip_id):
            async with self.semaphore:
                # í•´ë‹¹ ì „í‘œì˜ ê¸°ëŒ€ ê°’ ê°€ì ¸ì˜¤ê¸°
                expected_values = expected_values_map.get(slip_id) if expected_values_map else None
                return await self.process_single_pdf_async(pdf_path, slip_id, extraction_mode, expected_values)

        tasks = []
        for slip_folder in slip_folders:
            slip_path = os.path.join(split_dir, slip_folder)
            if not os.path.isdir(slip_path):
                continue
                
            slip_id = slip_folder
            
            # í•„í„°ë§: target_idsê°€ ìˆê³ , í˜„ì¬ slip_idê°€ ê·¸ ì•ˆì— ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if target_ids is not None and slip_id not in target_ids:
                continue
            
            # â˜… ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            current_slip += 1
            if progress_callback:
                progress_callback(current_slip - 1, total_slips, slip_id, f"ì¶”ì¶œ ì¤‘: {slip_id}")
            
            # âœ… OPTIMIZED: Check extraction_targets/ subfolder first
            extraction_targets_path = os.path.join(slip_path, 'extraction_targets')
            
            if os.path.exists(extraction_targets_path) and os.path.isdir(extraction_targets_path):
                # New structure: only scan extraction_targets/
                pdf_files = [f for f in os.listdir(extraction_targets_path) if f.lower().endswith('.pdf')]
                print(f"  âœ… {slip_id}: {len(pdf_files)} extraction target(s) found")
                
                for pdf_file in pdf_files:
                    pdf_path = os.path.join(extraction_targets_path, pdf_file)
                    tasks.append(sem_task(pdf_path, slip_id))
            else:
                # Fallback: Old structure - scan root folder
                # (This ensures backward compatibility with existing projects)
                pdf_files = [f for f in os.listdir(slip_path) if f.lower().endswith('.pdf')]
                print(f"  âš ï¸  {slip_id}: Using old structure, {len(pdf_files)} file(s) found")
                
                for pdf_file in pdf_files:
                    pdf_path = os.path.join(slip_path, pdf_file)
                    tasks.append(sem_task(pdf_path, slip_id))
        
        print(f"ğŸ“Š ì´ {len(tasks)}ê°œ íŒŒì¼ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘... (extraction_targetsë§Œ ìŠ¤ìº”)")
        
        # ë³‘ë ¬ ì‹¤í–‰
        try:
            results = await asyncio.gather(*tasks)
        except Exception as e:
            print(f"[CRITICAL ERROR] asyncio.gather failed: {e}")
            import traceback
            traceback.print_exc()
            with open("crash_log_engine.txt", "a") as f:
                f.write(f"[{datetime.now()}] Crash in smart_extraction_engine gather: {e}\n")
                f.write(traceback.format_exc())
            raise e
        
        # ê²°ê³¼ ì§‘ê³„ (ì „í‘œë³„ë¡œ ë¬¶ê¸°)
        slip_results_map = {}
        
        for res in results:
            slip_id = res.get('slip_id')
            if not slip_id: continue
            
            if slip_id not in slip_results_map:
                slip_results_map[slip_id] = {
                    "slip_id": slip_id,
                    "documents": [],
                    "source": "pdf_ocr"
                }
            
            if 'documents' in res:
                slip_results_map[slip_id]['documents'].extend(res['documents'])
        
        return list(slip_results_map.values())
